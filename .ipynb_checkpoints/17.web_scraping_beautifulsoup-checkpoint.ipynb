{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c7690cd",
   "metadata": {},
   "source": [
    "# Web Scraping con bs4 (BeautifulSoup)\n",
    "\n",
    "**`BeautifulSoup`** es una biblioteca de python para extraer contenido de ficheros **`HTML`** y **`XML`**.\n",
    "\n",
    "**`requests`** es una libreria que maneja los **requests** (o peticiones) de **HTTP** de una forma sencilla.\n",
    "\n",
    "**`bs4 : pip install beautifulsoup4`**\n",
    "\n",
    "**`requests : pip install requests`**\n",
    "\n",
    "Primero usamos **`requests`** para tener \"acceso\" a las páginas web, luego usamos **`BeautifulSoup`** para extraer la información del **`HTML`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cf30de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb539d32",
   "metadata": {},
   "source": [
    "### requests\n",
    "\n",
    "**`requests.get()`** toma un **`url`** (o enlace) y retorna la **\"respuesta\"** del servidor. Este nuevo objeto también extrae el código **`HTML`** del **`url`**.\n",
    "\n",
    "**ADVERTENCIA: CADA \"REQUEST\" TOMA UN TIEMPO DE RESPUESTA, POR LO QUE SI INTENTAMOS HACER MUCHOS \"REQUESTS\" EN UN PLAZO CORTO DE TIEMPO NUESTRO IP SERÁ BANEADO DE LA PAGINA WEB PARA EVITAR COLAPSAR LA PÁGINA WEB. EN EL PEOR DE LOS CASOS LA PÁGINA RECIBIRÁ TANTOS \"REQUESTS\" QUE COLAPSARÁ Y LA DEJAREMOS FUERA DE SERVICIO.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e3e36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://google.com/\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "print(response)\n",
    "\n",
    "print(bool(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278dcb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# El atributo .text retorna el HTML de la página \n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c7cb4c",
   "metadata": {},
   "source": [
    "### BeautifulSoup\n",
    "\n",
    "Si la respuesta del **`requests`** es positiva podemos pasar este objeto **`requests.get()`** a **`BeautifulSoup`** para que nos ayude a filtrar la información y extraerla más fácil. \n",
    "\n",
    "Los métodos más comunes de **`BeautifulSoup`** son:\n",
    "\n",
    "|Método           |Descripción                                                                                        |\n",
    "|-----------------|---------------------------------------------------------------------------------------------------|\n",
    "|**`.body`**      | Retorna el contenido dentro de la etiqueta **`body`**.                                            |\n",
    "|**`.title`**     | Retorna el titulo del **HTML**.                                                                   |\n",
    "|**`.find()`**    | Busca en el **HTML** y retorna la primera ocurrencia del filtro en un objeto **`bs4`**.           |\n",
    "|**`.find_all()`**| Busca en el **HTML** y retorna todas las ocurrencias del filtro en una lista de objetos **`bs4`**.|\n",
    "|**`.text`**      | Retorna el texto de un objeto **`bs4`** en un **`str`**.                                          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da58052",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f9b238",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77030637",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620410a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filmaffinity\n",
    "\n",
    "url = \"https://www.filmaffinity.com/es/topcat.php?id=new_th_es\"\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0bc1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896d26c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esto retorna un objeto bs4\n",
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef9358b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con .text lo convertimos a str\n",
    "\n",
    "soup.title.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e311dae",
   "metadata": {},
   "source": [
    "### Filtros con bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaa6027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontrar la primera etiqueta h1\n",
    "\n",
    "soup.find(\"h1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f0ddaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con .find_all() encontramos todas las etiquetas y nos retorna una lista con objetos bs4\n",
    "\n",
    "soup.find_all(\"h1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6535a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encuentra la primera etiqueta h3\n",
    "\n",
    "soup.find(\"h3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0464c52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Encuentra todas las etiquetas h3\n",
    "\n",
    "soup.find_all(\"h3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4ea01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(soup.find_all(\"h3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628eaf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos encadenar filtros siempre que nos retorne un objeto bs4\n",
    "\n",
    "soup.find(\"h3\").find(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9abb9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(\"h3\").find(\"a\")[\"href\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a8e9f6",
   "metadata": {},
   "source": [
    "Cuando veamos una etiqueta con **class** en ella, podemos usarla para hacer un filtro más específico.\n",
    "```html\n",
    "<div class=\"duration\">\n",
    "```\n",
    "\n",
    "Como en python **`class`** es una palabra reservada, debemos usar **`class_`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea74eda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(\"div\", class_ = \"duration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf00a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(\"div\", class_ = \"duration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24f3397",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(\"div\", class_ = \"avg-rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a1fe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(\"div\", class_ = \"avg-rating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a133304",
   "metadata": {},
   "source": [
    "Otra forma de usar estos filtros de **`bs4`** es atraves del parametro **`attrs`** de un diccionario.\n",
    "\n",
    "Este diccionario tendrá como llave el nombre de las etiquetas y como valor el valor asociado a esa etiqueta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3437d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "dicc_bs4 = {\"class\" : \"avg-rating\"}\n",
    "\n",
    "soup.find(\"div\", attrs = dicc_bs4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6ce966",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(\"div\", attrs = dicc_bs4).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23494333",
   "metadata": {},
   "outputs": [],
   "source": [
    "dicc_bs4 = {\"class\" : \"avg-rating\"}\n",
    "\n",
    "soup.find_all(\"div\", attrs = dicc_bs4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b3d14b",
   "metadata": {},
   "source": [
    "Utilizando bucles podemos iterar sobre los **`.find_all()`** y sacar información de forma automatizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8381dd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En la etiqueta h3 tenemos el url y el titulo de la pelicual\n",
    "\n",
    "soup.find_all(\"h3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e55366f",
   "metadata": {},
   "outputs": [],
   "source": [
    "titulos = list()\n",
    "\n",
    "urls = list()\n",
    "\n",
    "for bs in soup.find_all(\"h3\"):\n",
    "    titulo = bs.find(\"a\").text\n",
    "    url = bs.find(\"a\")[\"href\"]\n",
    "    \n",
    "    titulos.append(titulo)\n",
    "    urls.append(url)\n",
    "    \n",
    "    \n",
    "for t, u in zip(titulos, urls):\n",
    "    print(t, u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3699a764",
   "metadata": {},
   "source": [
    "Toda esta información se puede agrupar en un DataFrame y convertir a **`.csv`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c9a6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39f0062",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "df[\"titulo\"] = titulos\n",
    "\n",
    "df[\"urls\"] = urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53c54bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4704af72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_or_buf = Nombre del archivo terminado en .csv\n",
    "# index = False para que no se guarde el indice como nueva columna en el csv\n",
    "# sep = \",\" para que el separador sea \",\", se puede cambiar por cualquier otro separador.\n",
    "\n",
    "df.to_csv(path_or_buf = \"filmaffinity.csv\", index = False, sep = \",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9881539",
   "metadata": {},
   "source": [
    "### Agregar al DataFrame la duración de cada pelicula y su puntaje:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37766958",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4c7e65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9131c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a7cf80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782569bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d17a665b",
   "metadata": {},
   "source": [
    "### Información de 1 pelicula:\n",
    "\n",
    "Vamos a entrar al primer **`url`** y sacar toda su información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb45c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.filmaffinity.com/es/film491812.html\"\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618df1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Titulo\n",
    "\n",
    "soup.find(\"dd\").text\n",
    "\n",
    "# soup.find(\"dd\").text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a13ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Año\n",
    "\n",
    "dict_bs4 = {\"itemprop\" : \"datePublished\"}\n",
    "\n",
    "soup.find(\"dd\", attrs = dict_bs4).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117045ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duracion\n",
    "\n",
    "dict_bs4 = {\"itemprop\" : \"duration\"}\n",
    "\n",
    "soup.find(\"dd\", attrs = dict_bs4).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061a21f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pais\n",
    "\n",
    "dict_bs4 = {\"id\" : \"country-img\"}\n",
    "\n",
    "soup.find(\"span\", attrs = dict_bs4).find(\"img\")[\"alt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2012796d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guion (Aqui es más complicado por como esta estructurada la página)\n",
    "\n",
    "soup.find_all(\"div\", class_ = \"credits\")[1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14761266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Musica (Aqui es más complicado por como esta estructurada la página)\n",
    "\n",
    "soup.find_all(\"div\", class_ = \"credits\")[2].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff5f5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fotografia (Aqui es más complicado por como esta estructurada la página)\n",
    "\n",
    "soup.find_all(\"div\", class_ = \"credits\")[3].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574f456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reparto (Aqui es más complicado por como esta estructurada la página)\n",
    "\n",
    "soup.find_all(\"div\", class_ = \"credits\")[4].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ae258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Productora \n",
    "\n",
    "\" \".join([t.text for t in soup.find(\"dd\", class_ = \"card-producer\").find_all(\"span\", class_ = \"nb\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125d9012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Géneros\n",
    "\n",
    "\" \".join(soup.find(\"dd\", class_ = \"card-genres\").text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921c3150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grupos\n",
    "\n",
    "\" \".join(soup.find(\"dd\", attrs = {\"style\" : \"position: relative;\"}).text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7d22cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sinopsis\n",
    "\n",
    "soup.find(\"dd\", attrs = {\"itemprop\" : \"description\"}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91884a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dc462a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame\n",
    "\n",
    "titulos = list()\n",
    "años = list()\n",
    "duraciones = list()\n",
    "paises = list()\n",
    "direcciones = list()\n",
    "guiones = list()\n",
    "musicas = list()\n",
    "fotografias = list()\n",
    "repartos = list()\n",
    "productoras = list()\n",
    "generos = list()\n",
    "grupos = list()\n",
    "sinopsises = list()\n",
    "\n",
    "for url in df[\"urls\"]:\n",
    "    \n",
    "    print(url)\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    titulo = soup.find(\"dd\").text.strip()\n",
    "    año = soup.find(\"dd\", attrs = {\"itemprop\" : \"datePublished\"}).text\n",
    "    duracion = soup.find(\"dd\", attrs = {\"itemprop\" : \"duration\"}).text\n",
    "    pais = soup.find(\"span\", attrs = {\"id\" : \"country-img\"}).find(\"img\")[\"alt\"]\n",
    "    direccion = soup.find_all(\"div\", class_ = \"credits\")[1].text\n",
    "    guion = soup.find_all(\"div\", class_ = \"credits\")[2].text\n",
    "    musica = soup.find_all(\"div\", class_ = \"credits\")[3].text\n",
    "    fotografia = soup.find_all(\"div\", class_ = \"credits\")[3].text\n",
    "    reparto = soup.find_all(\"div\", class_ = \"credits\")[4].text\n",
    "    productora = \" \".join([t.text for t in soup.find(\"dd\", class_ = \"card-producer\").find_all(\"span\", class_ = \"nb\")])\n",
    "    genero = \" \".join(soup.find(\"dd\", class_ = \"card-genres\").text.split())\n",
    "    grupo = \" \".join(soup.find(\"dd\", attrs = {\"style\" : \"position: relative;\"}).text.split())\n",
    "    sinopsis = soup.find(\"dd\", attrs = {\"itemprop\" : \"description\"}).text\n",
    "    \n",
    "    titulos.append(titulo)\n",
    "    años.append(año)\n",
    "    duraciones.append(duracion)\n",
    "    paises.append(pais)\n",
    "    direcciones.append(direccion)\n",
    "    guiones.append(guion)\n",
    "    musicas.append(musica)\n",
    "    fotografias.append(fotografia)\n",
    "    repartos.append(reparto)\n",
    "    productoras.append(productora)\n",
    "    generos.append(genero)\n",
    "    grupos.append(grupo)\n",
    "    sinopsises.append(sinopsis)\n",
    "    \n",
    "    sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c7c832",
   "metadata": {},
   "source": [
    "Muchas veces sucede que no todas las páginas tienen la misma información, por lo que tenemos que asegurarnos de que si no existe un dato lo sustituimos con un np.nan. Por eso podemos usar:\n",
    "```python\n",
    "try:\n",
    "    \n",
    "except:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352d926d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame\n",
    "\n",
    "titulos = list()\n",
    "años = list()\n",
    "duraciones = list()\n",
    "paises = list()\n",
    "direcciones = list()\n",
    "guiones = list()\n",
    "musicas = list()\n",
    "fotografias = list()\n",
    "repartos = list()\n",
    "productoras = list()\n",
    "generos = list()\n",
    "grupos = list()\n",
    "sinopsises = list()\n",
    "\n",
    "for url in df[\"urls\"]:\n",
    "    \n",
    "    print(url)\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "#### Titulo #######################################################################################################   \n",
    "\n",
    "    try:\n",
    "        titulo = soup.find(\"dd\").text.strip()\n",
    "        \n",
    "    except:\n",
    "        titulo = np.nan\n",
    "        \n",
    "#### Año ##########################################################################################################   \n",
    "\n",
    "    try:\n",
    "        año = soup.find(\"dd\", attrs = {\"itemprop\" : \"datePublished\"}).text\n",
    "        \n",
    "    except:\n",
    "        año = np.nan\n",
    "\n",
    "#### Duracion #####################################################################################################\n",
    "\n",
    "    try:\n",
    "        duracion = soup.find(\"dd\", attrs = {\"itemprop\" : \"duration\"}).text\n",
    "        \n",
    "    except:\n",
    "        duracion = np.nan\n",
    "\n",
    "#### Pais #########################################################################################################\n",
    "\n",
    "    try:\n",
    "        pais = soup.find(\"span\", attrs = {\"id\" : \"country-img\"}).find(\"img\")[\"alt\"]\n",
    "        \n",
    "    except:\n",
    "        pais = np.nan\n",
    "\n",
    "#### Direccion ####################################################################################################\n",
    "\n",
    "    try:\n",
    "        direccion = soup.find_all(\"div\", class_ = \"credits\")[1].text\n",
    "        \n",
    "    except:\n",
    "        direccion = np.nan\n",
    "\n",
    "#### Guion ########################################################################################################\n",
    "\n",
    "    try:\n",
    "        guion = soup.find_all(\"div\", class_ = \"credits\")[2].text\n",
    "        \n",
    "    except:\n",
    "        guion = np.nan\n",
    "\n",
    "#### Musica #######################################################################################################\n",
    "\n",
    "    try:\n",
    "        musica = soup.find_all(\"div\", class_ = \"credits\")[3].text\n",
    "        \n",
    "    except:\n",
    "        musica = np.nan\n",
    "\n",
    "#### Fotografia ###################################################################################################\n",
    "\n",
    "    try:\n",
    "        fotografia = soup.find_all(\"div\", class_ = \"credits\")[3].text\n",
    "        \n",
    "    except:\n",
    "        fotografia = np.nan\n",
    "\n",
    "#### Reparto ######################################################################################################\n",
    "\n",
    "    try:\n",
    "        reparto = soup.find_all(\"div\", class_ = \"credits\")[4].text\n",
    "        \n",
    "    except:\n",
    "        reparto = np.nan\n",
    "\n",
    "#### Productora ###################################################################################################\n",
    "\n",
    "    try:\n",
    "        productora = \" \".join([t.text for t in soup.find(\"dd\", class_ = \"card-producer\").find_all(\"span\", class_ = \"nb\")])\n",
    "        \n",
    "    except:\n",
    "        productora = np.nan\n",
    "\n",
    "#### Genero #######################################################################################################\n",
    "\n",
    "    try:\n",
    "        genero = \" \".join(soup.find(\"dd\", class_ = \"card-genres\").text.split())\n",
    "        \n",
    "    except:\n",
    "        genero = np.nan\n",
    "\n",
    "#### Grupo ########################################################################################################\n",
    "\n",
    "    try:\n",
    "        grupo = \" \".join(soup.find(\"dd\", attrs = {\"style\" : \"position: relative;\"}).text.split())\n",
    "        \n",
    "    except:\n",
    "        grupo = np.nan\n",
    "\n",
    "#### Sinopsis #####################################################################################################\n",
    "\n",
    "    try:\n",
    "        sinopsis = soup.find(\"dd\", attrs = {\"itemprop\" : \"description\"}).text\n",
    "        \n",
    "    except:\n",
    "        sinopsis = np.nan\n",
    "        \n",
    "##################################################################################################################\n",
    "\n",
    "    # appends\n",
    "    \n",
    "    titulos.append(titulo)\n",
    "    años.append(año)\n",
    "    duraciones.append(duracion)\n",
    "    paises.append(pais)\n",
    "    direcciones.append(direccion)\n",
    "    guiones.append(guion)\n",
    "    musicas.append(musica)\n",
    "    fotografias.append(fotografia)\n",
    "    repartos.append(reparto)\n",
    "    productoras.append(productora)\n",
    "    generos.append(genero)\n",
    "    grupos.append(grupo)\n",
    "    sinopsises.append(sinopsis)\n",
    "    \n",
    "    sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed14af17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_peliculas = pd.DataFrame()\n",
    "\n",
    "df_peliculas[\"titulo\"] = titulos\n",
    "df_peliculas[\"año\"] = años\n",
    "df_peliculas[\"duracion\"] = duraciones\n",
    "df_peliculas[\"pais\"] = paises\n",
    "df_peliculas[\"direccion\"] = direcciones\n",
    "df_peliculas[\"guion\"] = guiones\n",
    "df_peliculas[\"fotografia\"] = fotografias\n",
    "df_peliculas[\"reparto\"] = repartos\n",
    "df_peliculas[\"productora\"] = productoras\n",
    "df_peliculas[\"genero\"] = generos\n",
    "df_peliculas[\"grupo\"] = grupos\n",
    "df_peliculas[\"sinopsis\"] = sinopsises\n",
    "\n",
    "df_peliculas[\"url\"] = df[\"urls\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce077f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_peliculas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3b80a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_peliculas.to_csv(\"info_peliculas.csv\", index = False, sep = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b839973a",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
